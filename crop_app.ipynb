{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Crop Disease Classification App\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Create the datasets (train,test,valid from the original dataset)\n",
    "\n",
    "### download the original dataset\n",
    "download the initial dataset from this url :https://zenodo.org/record/1204914/files/plantvillage_deeplearning_paper_dataset.7z\n",
    "In the code cell below, we import a dataset of crop images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `crop_names` - list of string-valued crop names for translating labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Renaming the file in the original dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the files in all_data_set so they allways have as prefix the directory name related\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "for f in os.listdir(all_data_dir):\n",
    "  foldername = f\n",
    "  for file in os.listdir( all_data_dir + '/' + f):\n",
    "    oldfile= Path(all_data_dir + '/' + f + '/' + file)\n",
    "    if len(file)>50:\n",
    "      index = file.rfind('_')\n",
    "      \n",
    "      #print (file[index:]) \n",
    "      #print (foldername + file[index:])\n",
    "      newfile = Path(all_data_dir + '/' + f + '/' + foldername + file[index:])\n",
    "      print (newfile)\n",
    "      p=oldfile\n",
    "      p.rename(newfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Creation of the Train/Valid/Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, testing_data_pct):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    num_training_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "\n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            if np.random.rand(1) < testing_data_pct:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil, sys\n",
    "import numpy as np\n",
    "all_data_dir='../DataSet/alldataset'\n",
    "training_data_dir='../DataSet/traindataset'\n",
    "training_data_dir2='../DataSet/train'\n",
    "testing_data_dir='../DataSet/test'\n",
    "valid_data_dir='../DataSet/valid'\n",
    "\n",
    "#split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, 0.3)\n",
    "#split_dataset_into_test_and_train_sets(training_data_dir, training_data_dir2, valid_data_dir, 0.3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Loading of the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18844\n",
      "There are 39 total crop categories.\n",
      "There are 18844 training crop images.\n",
      "There are 8063 validation crop images.\n",
      "There are 11664 test crop images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    crop_files = np.array(data['filenames'])\n",
    "    crop_targets = np_utils.to_categorical(np.array(data['target']), 39)\n",
    "    return crop_files, crop_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../DataSet/train')\n",
    "valid_files, valid_targets = load_dataset('../DataSet/valid')\n",
    "test_files, test_targets = load_dataset('../DataSet/test')\n",
    "\n",
    "print (len(train_targets))\n",
    "\n",
    "# load list of crop names\n",
    "crop_names = [item[17:-1] for item in sorted(glob(\"../DataSet/train/*/\"))]\n",
    "#for item in sorted(glob(\"../DataSet/train/*/\")):\n",
    "# print statistics about the dataset\n",
    "print('There are %d total crop categories.' % len(crop_names))\n",
    "# print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training crop images.' % len(train_files))\n",
    "print('There are %d validation crop images.' % len(valid_files))\n",
    "print('There are %d test crop images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18844/18844 [00:32<00:00, 578.88it/s]\n",
      "100%|██████████| 8063/8063 [00:12<00:00, 664.72it/s]\n",
      "100%|██████████| 11664/11664 [00:18<00:00, 630.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "\n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 5: Use the VGG16 (Transfer Learning)\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = training_data_dir2\n",
    "validation_data_dir = valid_data_dir\n",
    "test_data_dir = testing_data_dir\n",
    "\n",
    "# number of epochs to train top model\n",
    "epochs = 50\n",
    "# batch size used by flow_from_directory and predict_generator\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    if False:\n",
    "      generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "      print(len(generator.filenames))\n",
    "      print(generator.class_indices)\n",
    "      print(len(generator.class_indices))\n",
    "\n",
    "      nb_train_samples = len(generator.filenames)\n",
    "      num_classes = len(generator.class_indices)\n",
    "\n",
    "      predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "\n",
    "      bottleneck_features_train = model.predict_generator(\n",
    "        generator, predict_size_train)\n",
    "\n",
    "      np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "      generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "      nb_validation_samples = len(generator.filenames)\n",
    "\n",
    "      predict_size_validation = int(\n",
    "        math.ceil(nb_validation_samples / batch_size))\n",
    "\n",
    "      bottleneck_features_validation = model.predict_generator(\n",
    "        generator, predict_size_validation)\n",
    "\n",
    "      np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "      generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "      nb_test_samples = len(generator.filenames)\n",
    "\n",
    "      predict_size_test = int(\n",
    "        math.ceil(nb_test_samples / batch_size))\n",
    "\n",
    "      bottleneck_features_test = model.predict_generator(\n",
    "        generator, predict_size_test)\n",
    "\n",
    "      np.save('bottleneck_features_test.npy',\n",
    "            bottleneck_features_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using Bottleneck Features for Multi-Class Classification in Keras\n",
    "We use this technique to build powerful (high accuracy without overfitting) Image Classification systems with small\n",
    "amount of training data.\n",
    "'''\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_model_weights_path = 'saved_models/weights.best.vgg16.hdf5'\n",
    "train_data_dir = training_data_dir2\n",
    "validation_data_dir = valid_data_dir\n",
    "test_data_dir = testing_data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18844\n",
      "8063\n",
      "11664\n"
     ]
    }
   ],
   "source": [
    "train_VGG16 = np.load('bottleneck_features_train.npy')\n",
    "valid_VGG16 = np.load('bottleneck_features_validation.npy')\n",
    "test_VGG16 =  np.load('bottleneck_features_test.npy')\n",
    "\n",
    "\n",
    "print(len(train_VGG16))\n",
    "print(len(valid_VGG16))\n",
    "print(len(test_VGG16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG16 model as a fixed feature extractor, where the last convolutional output of VGG16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each crop category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 39)                10023     \n",
      "=================================================================\n",
      "Total params: 6,432,807\n",
      "Trainable params: 6,432,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# number of epochs to train top model\n",
    "epochs = 10\n",
    "# batch size used by flow_from_directory and predict_generator\n",
    "batch_size = 16\n",
    "\n",
    "#datagen_top = ImageDataGenerator(rescale=1. / 255)\n",
    "#generator_top = datagen_top.flow_from_directory(\n",
    "#    train_data_dir,\n",
    "#    target_size=(img_width, img_height),\n",
    "#    batch_size=batch_size,\n",
    "#    class_mode='categorical',\n",
    "#    shuffle=False)\n",
    "\n",
    "#nb_train_samples = len(generator_top.filenames)\n",
    "#num_classes = len(generator_top.class_indices)\n",
    "# save the class indices to use use later in predictions\n",
    "#np.save('class_indices.npy', generator_top.class_indices)\n",
    "\n",
    "#generator_top = datagen_top.flow_from_directory(\n",
    "#    validation_data_dir,\n",
    "#    target_size=(img_width, img_height),\n",
    "#    batch_size=batch_size,\n",
    "#    class_mode=None,\n",
    "#    shuffle=False)\n",
    "\n",
    "#nb_validation_samples = len(generator_top.filenames)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_VGG16.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='RMSprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=\"RMSprop\", loss=\"mean_squared_error\", metrics=[\"accuracy\"] )\n",
    "#from keras.optimizers import SGD\n",
    "#model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=top_model_weights_path, verbose=1, save_best_only=True)\n",
    "\n",
    "history=model.fit(train_VGG16, train_targets,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(valid_VGG16, valid_targets),callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "model.save('crop_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model.add(top_model)\n",
    "We then proceed to freeze all convolutional layers up to the last convolutional block:\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "Finally, we start training the whole thing, with a very slow learning rate:\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "model.load_weights(top_model_weights_path)\n",
    "VGG16_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "## TODO: Visualize the training and validation loss of your neural network\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "(eval_loss, eval_accuracy) = model.evaluate(\n",
    "    valid_VGG16, valid_targets, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
    "print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD MODEL FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 13, 13, 256)       131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 39)                10023     \n",
      "=================================================================\n",
      "Total params: 184,791\n",
      "Trainable params: 184,791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import deep learning resources from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers import Conv2D, GlobalAveragePooling2D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "#def create_model():\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=2, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18844 samples, validate on 8063 samples\n",
      "Epoch 1/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9768\n",
      "Epoch 00001: val_loss improved from inf to 0.08007, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 67s 4ms/step - loss: 0.0826 - acc: 0.9768 - val_loss: 0.0801 - val_acc: 0.9770\n",
      "Epoch 2/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9835\n",
      "Epoch 00002: val_loss improved from 0.08007 to 0.03878, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0495 - acc: 0.9835 - val_loss: 0.0388 - val_acc: 0.9869\n",
      "Epoch 3/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9877\n",
      "Epoch 00003: val_loss did not improve\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0355 - acc: 0.9877 - val_loss: 0.0727 - val_acc: 0.9771\n",
      "Epoch 4/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9900\n",
      "Epoch 00004: val_loss improved from 0.03878 to 0.02937, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0287 - acc: 0.9900 - val_loss: 0.0294 - val_acc: 0.9898\n",
      "Epoch 5/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9913\n",
      "Epoch 00005: val_loss improved from 0.02937 to 0.02640, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0243 - acc: 0.9913 - val_loss: 0.0264 - val_acc: 0.9910\n",
      "Epoch 6/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9924\n",
      "Epoch 00006: val_loss did not improve\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0216 - acc: 0.9924 - val_loss: 0.0360 - val_acc: 0.9881\n",
      "Epoch 7/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9934\n",
      "Epoch 00007: val_loss improved from 0.02640 to 0.02206, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0189 - acc: 0.9934 - val_loss: 0.0221 - val_acc: 0.9924\n",
      "Epoch 8/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9940\n",
      "Epoch 00008: val_loss did not improve\n",
      "18844/18844 [==============================] - 66s 3ms/step - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0480 - val_acc: 0.9855\n",
      "Epoch 9/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9945\n",
      "Epoch 00009: val_loss improved from 0.02206 to 0.01650, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "18844/18844 [==============================] - 66s 4ms/step - loss: 0.0158 - acc: 0.9945 - val_loss: 0.0165 - val_acc: 0.9943\n",
      "Epoch 10/10\n",
      "18840/18844 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9951\n",
      "Epoch 00010: val_loss did not improve\n",
      "18844/18844 [==============================] - 66s 4ms/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0290 - val_acc: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03b6105ac8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18844 images belonging to 39 classes.\n",
      "Found 8063 images belonging to 39 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have shape (39,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1cdc98e04b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         validation_steps=800 // batch_size)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# always save your weights after training or during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/Keras-2.1.3-py3.4.egg/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have shape (39,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights(top_model_weights_path)  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.7346%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')\n",
    "\n",
    "# get index of predicted crop for each image in test set\n",
    "crop_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(crop_predictions)==np.argmax(test_targets, axis=1))/len(crop_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_breed_fromscratch(img_path):\n",
    "    # obtain predicted vector\n",
    "    imgtensor=path_to_tensor(img_path)\n",
    "    predicted_vector = model.predict(imgtensor)\n",
    "    return crop_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/tomatoseptoria2.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/tomato-yellow-leaf-curl.JPG\n",
      "crop:Pepper_bell___healthy\n",
      "images/peachhealthy.JPG\n",
      "crop:Blueberry___healthy\n",
      "images/applescab.JPG\n",
      "crop:Pepper_bell___healthy\n",
      "images/blueberryhealthy.JPG\n",
      "crop:Blueberry___healthy\n",
      "images/tomatoseptoria.JPG\n",
      "crop:Tomato___Septoria_leaf_spot\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "from keras.preprocessing import image\n",
    "various_files = np.array(glob(\"images/*\"))\n",
    "random.shuffle(various_files)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "for image_path in various_files:\n",
    "    try:\n",
    "        print(image_path)  \n",
    "        cropname=predict_breed_fromscratch(image_path)\n",
    "        print(\"crop:{}\".format(cropname))\n",
    "        \n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict  with the VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = model.predict(bottleneck_feature)\n",
    "    return crop_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/tomatoseptoria2.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/tomato-yellow-leaf-curl.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/peachhealthy.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/applescab.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/blueberryhealthy.JPG\n",
      "crop:Apple_Frogeye_Spot\n",
      "images/tomatoseptoria.JPG\n",
      "crop:Apple_Frogeye_Spot\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "from keras.preprocessing import image\n",
    "various_files = np.array(glob(\"images/*\"))\n",
    "random.shuffle(various_files)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "for image_path in various_files:\n",
    "    try:\n",
    "        print(image_path)  \n",
    "        cropname=VGG16_predict_breed(image_path)\n",
    "        print(\"crop:{}\".format(cropname))\n",
    "        \n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fine tuning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
